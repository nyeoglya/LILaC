import os
from pathlib import Path

from utils import get_clean_savepath
from utils_mmqa import mmqa_get_clean_wikidocs_titles
from preprocess import MMQA_PATH, MMQA_CRAWL_IMAGE_FOLDER, MMQA_PARSE_JSON_FOLDER
from crawler.wiki import BatchWikiImageCrawler


def get_actual_pending_downloads(target_url_list: list, save_folderpath: str):
    virtual_existing_names = set()
    
    actual_files = [f for f in os.listdir(save_folderpath) if os.path.isfile(os.path.join(save_folderpath, f))]
    
    for real_name in actual_files:
        stem = Path(real_name).stem
        ext = Path(real_name).suffix.lstrip('.')
        
        v_path = get_clean_savepath(save_folderpath, stem, ext)
        v_name = os.path.basename(v_path)
        virtual_existing_names.add(v_name)

    pending_urls = []
    
    target_expected_names = set()
    for url in target_url_list:
        original_fn = url.split('/')[-1]
        name_part, ext_part = os.path.splitext(original_fn)
        target_path = get_clean_savepath(save_folderpath, name_part, ext_part.lstrip('.'))
        target_expected_names.add(os.path.basename(target_path))

    orphaned_files = []

    for v_name in virtual_existing_names:
        if v_name not in target_expected_names:
            orphaned_files.append(v_name)

    print(f"ğŸ“Š ì—­ë°©í–¥ ëŒ€ì¡° ê²°ê³¼")
    print(f"- í˜„ì¬ íƒ€ê²Ÿ(JSON) ì´ë¯¸ì§€: {len(target_expected_names)}ê°œ")
    print(f"- ë°±ì—… í´ë” ë‚´ ìœ ë‹ˆí¬ íŒŒì¼: {len(virtual_existing_names)}ê°œ")
    print(f"- íƒ€ê²Ÿ ëª©ë¡ì— ì—†ëŠ” ì‰ì—¬ íŒŒì¼: {len(orphaned_files)}ê°œ")
    print(orphaned_files[:10])
    
    return pending_urls

mmqa_wiki_doc_title_list = mmqa_get_clean_wikidocs_titles(MMQA_PATH)

batch_image_crawler = BatchWikiImageCrawler(MMQA_CRAWL_IMAGE_FOLDER)
batch_image_crawler.set_clean_imglinks_from_folder(MMQA_PARSE_JSON_FOLDER)

get_actual_pending_downloads(batch_image_crawler.image_data_url_list, "/dataset/backup/mmqa_image")

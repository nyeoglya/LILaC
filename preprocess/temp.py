import os
from pathlib import Path

from utils import get_clean_savepath
from utils_mmqa import mmqa_get_clean_wikidocs_titles
from preprocess import MMQA_PATH, MMQA_CRAWL_IMAGE_FOLDER, MMQA_PARSE_JSON_FOLDER
from crawler.wiki import BatchWikiImageCrawler


def get_actual_pending_downloads(target_url_list: list, save_folderpath: str):
    """
    1. ë¡œì»¬ íŒŒì¼ ëª©ë¡ì„ ì½ì–´ì˜¨ë‹¤.
    2. ë¡œì»¬ íŒŒì¼ëª…ì„ ê°€ìƒìœ¼ë¡œ clean ê·œì¹™ì— ë§ì¶° ë³€í™˜í•´ë³¸ë‹¤.
    3. ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•  URLë“¤ì„ clean ê·œì¹™ì— ë§ì¶˜ ì´ë¦„ê³¼ ëŒ€ì¡°í•œë‹¤.
    4. ì´ë¯¸ ìˆëŠ” ë…€ì„ë“¤ì„ ì œì™¸í•œ 'ì§„ì§œ ì‹ ê·œ' ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    
    # 1. ë¡œì»¬ì— ìˆëŠ” ì‹¤ì œ íŒŒì¼ëª…ë“¤ì„ ë‚´ ê·œì¹™(clean)ìœ¼ë¡œ ê°€ìƒ ë³€í™˜í•œ ì§‘í•© ìƒì„±
    # ì‹¤ì œ íŒŒì¼ì€ ì´ë¦„ì´ ì•ˆ ë°”ë€Œê³ , ë©”ëª¨ë¦¬ ì•ˆì—ì„œë§Œ ë°”ë€ ì´ë¦„ìœ¼ë¡œ ì¸ì‹ë¨
    virtual_existing_names = set()
    
    actual_files = [f for f in os.listdir(save_folderpath) if os.path.isfile(os.path.join(save_folderpath, f))]
    
    for real_name in actual_files:
        stem = Path(real_name).stem
        ext = Path(real_name).suffix.lstrip('.')
        
        # ê°€ìƒìœ¼ë¡œ ê²½ë¡œë¥¼ ìƒì„±í•œ ë’¤ ì´ë¦„ë§Œ ì¶”ì¶œ
        v_path = get_clean_savepath(save_folderpath, stem, ext)
        v_name = os.path.basename(v_path)
        virtual_existing_names.add(v_name)

    # 2. ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ URLë“¤ ì¤‘ ì—†ëŠ” ê²ƒë§Œ ê³¨ë¼ë‚´ê¸°
    pending_urls = []
    
    target_expected_names = set()
    for url in target_url_list:
        original_fn = url.split('/')[-1]
        name_part, ext_part = os.path.splitext(original_fn)
        target_path = get_clean_savepath(save_folderpath, name_part, ext_part.lstrip('.'))
        target_expected_names.add(os.path.basename(target_path))

    # 2. ë¡œì»¬ íŒŒì¼ë“¤(virtual_existing_names)ì„ í•˜ë‚˜ì”© ê²€ì‚¬
    # ë¡œì»¬ì—ëŠ” ìˆì§€ë§Œ, í˜„ì¬ íƒ€ê²Ÿ ëª©ë¡ì—ëŠ” ì—†ëŠ” íŒŒì¼ë“¤ì„ ì¶”ì¶œ
    orphaned_files = [] # í˜„ì¬ ëª©ë¡ì— ì—†ëŠ” ë¡œì»¬ íŒŒì¼ë“¤

    # virtual_existing_namesëŠ” ì´ì „ ë¡œì§ì—ì„œ ë§Œë“  'ë¡œì»¬ íŒŒì¼ì˜ clean ëª…ì¹­ ë¦¬ìŠ¤íŠ¸'ì…ë‹ˆë‹¤.
    for v_name in virtual_existing_names:
        if v_name not in target_expected_names:
            orphaned_files.append(v_name)

    print(f"ğŸ“Š ì—­ë°©í–¥ ëŒ€ì¡° ê²°ê³¼")
    print(f"- í˜„ì¬ íƒ€ê²Ÿ(JSON) ì´ë¯¸ì§€: {len(target_expected_names)}ê°œ")
    print(f"- ë°±ì—… í´ë” ë‚´ ìœ ë‹ˆí¬ íŒŒì¼: {len(virtual_existing_names)}ê°œ")
    print(f"- íƒ€ê²Ÿ ëª©ë¡ì— ì—†ëŠ” ì‰ì—¬ íŒŒì¼: {len(orphaned_files)}ê°œ")
    print(orphaned_files[:10])
    
    return pending_urls

mmqa_wiki_doc_title_list = mmqa_get_clean_wikidocs_titles(MMQA_PATH)

# Image Crawler
batch_image_crawler = BatchWikiImageCrawler(MMQA_CRAWL_IMAGE_FOLDER)
batch_image_crawler.set_clean_imglinks_from_folder(MMQA_PARSE_JSON_FOLDER)

get_actual_pending_downloads(batch_image_crawler.image_data_url_list, "/dataset/backup/mmqa_image")
